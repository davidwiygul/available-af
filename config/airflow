# airflow
# airflow PUBLIC_IP_ADDRESS

# installs and configures airflow on given instance
# called by provision_webserver, provision_scheduler,
#     provision_multischeduler, and provision_worker
# refers to provision.config for various parameters
# also installs psycopg2, required for Python Postgres support
# Python Riak library is also installed,
#     but this is need only for our particular demo

#!/bin/bash


source provision.config

HEAD="pyamqp://$QUEUE_USER:$QUEUE_PWD@"
TAIL=":5672/$QUEUE_HOST"
BROKER_URL=$(echo $MQ_IP | sed "s|^|$HEAD|" | sed "s| |$HEAD|g" | sed "s|$|$TAIL|" | sed "s|,|$TAIL\\\;|g")

ssh -T -i $AWS_SSH_KEY ubuntu@$1 << HERE
    sudo apt-get install -y python3-pip

    python3 -m pip install riak

#    python3 -m pip install pipdeptree

    python3 -m pip install psycopg2-binary

#    python3 -m pip install werkzeug==0.16.0

    python3 -m pip install apache-airflow[postgres,celery]

    source .profile

   airflow initdb

    declare -A dict=(
        [dags_folder]=$EFS_PATH/dags
        [sql_alchemy_conn]=postgresql+psycopg2://$DB_USER:$DB_PWD@$DB_IP/$DATABASE
        [executor]=CeleryExecutor
        [broker_url]=$BROKER_URL
        [result_backend]=db+postgresql://$DB_USER:$DB_PWD@$DB_IP/$DATABASE
        [load_examples]=False
 	#[parallelism]=32
	#[dag_concurrency]=32
	#[worker_concurrency]=8
	#[worker_autoscale]=8,8
)

    for key in \${!dict[@]}; do
        value=\${dict[\$key]}
        sed -i "s|\(^\$key\).*|\1 = \$value|" ~/airflow/airflow.cfg
    done

    airflow initdb

    exit


HERE
